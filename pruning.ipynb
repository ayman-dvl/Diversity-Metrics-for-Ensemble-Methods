{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3010097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported keras model: cnn_model\n",
      "Imported sklearn model: decision_tree\n",
      "Imported sklearn model: kernel_svc\n",
      "Imported sklearn model: linear_svc\n",
      "Imported sklearn model: random_forest\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# convert models dict to list of tuples for VotingClassifier\u001b[39;00m\n\u001b[32m    138\u001b[39m models = [(name, model) \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items()]\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m score, model_list, scores, Q_stats = \u001b[43mprune_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Plot scores and Q_stats as time series\u001b[39;00m\n\u001b[32m    141\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mprune_ensemble\u001b[39m\u001b[34m(models, X, y)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# prune ensemble until no further improvement or max iterations reached\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m iterations < m_length:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# remove one model from the ensemble\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     score, removed, stat_Q = \u001b[43mprune_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     scores.append(score)\n\u001b[32m    119\u001b[39m     Q_stats.append(stat_Q)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mprune_round\u001b[39m\u001b[34m(models_in, X, y)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprune_round\u001b[39m(models_in, X, y):\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform a single round of pruning based on Q-statistic diversity\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     baseline_acc = \u001b[43mevaluate_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     baseline_Q = mean_Q_statistic(models_in, X, y)\n\u001b[32m     87\u001b[39m     best_score = baseline_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mevaluate_ensemble\u001b[39m\u001b[34m(models, X, y)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_ensemble\u001b[39m(models, X, y):\n\u001b[32m     37\u001b[39m \t\u001b[38;5;66;03m# check for no models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m()) == \u001b[32m0\u001b[39m:\n\u001b[32m     39\u001b[39m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m     40\u001b[39m \t\u001b[38;5;66;03m# create the ensemble\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# example of ensemble pruning for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "import os   \n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\treturn X, y\n",
    "\n",
    "# Charger les modÃ¨les depuis le dossier \"models\"\n",
    "def get_models():\n",
    "    \"\"\"Load models from the 'models' directory\"\"\"\n",
    "    models = {}\n",
    "    models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "    for file in os.listdir(models_folder):\n",
    "        model_name = file.split(\".\")[0]\n",
    "        file_extension = file.split(\".\")[-1]\n",
    "        if file_extension == \"pkl\":\n",
    "            models[model_name] = joblib.load(os.path.join(models_folder, file))\n",
    "            print(f\"Imported sklearn model: {model_name}\")\n",
    "        elif file_extension == \"keras\":\n",
    "            models[model_name] = load_model(os.path.join(models_folder, file))\n",
    "            print(f\"Imported keras model: {model_name}\")\n",
    "    return models\n",
    "\n",
    "# evaluate a list of models\n",
    "def evaluate_ensemble(models, X, y):\n",
    "\t# check for no models\n",
    "\tif len(models.items()) == 0:\n",
    "\t\treturn 0.0\n",
    "\t# create the ensemble\n",
    "\tensemble = VotingClassifier(estimators=models.items(), voting='soft')\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the ensemble\n",
    "\tscores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\t# return mean score\n",
    "\treturn mean(scores)\n",
    "\n",
    "def calculate_Q_statistic(predictions1, predictions2):\n",
    "    \"\"\"Calculate Q-statistic between two classifiers' predictions\"\"\"\n",
    "    N11 = sum((predictions1 == 1) & (predictions2 == 1))\n",
    "    N00 = sum((predictions1 == 0) & (predictions2 == 0))\n",
    "    N10 = sum((predictions1 == 1) & (predictions2 == 0))\n",
    "    N01 = sum((predictions1 == 0) & (predictions2 == 1))\n",
    "    \n",
    "    Q = (N11 * N00 - N10 * N01) / (N11 * N00 + N10 * N01 + 1e-10)\n",
    "    return Q\n",
    "\n",
    "def get_predictions(model, X, y):\n",
    "    \"\"\"Get binary predictions from a model\"\"\"\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred = model.predict_proba(X)[:,1] > 0.5\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "def mean_Q_statistic(models, X, y):\n",
    "    \"\"\"Calculate mean Q-statistic across all pairs of models\"\"\"\n",
    "    n_models = len(models)\n",
    "    if n_models < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    Q_values = []\n",
    "    predictions = [get_predictions(model, X, y) for model in models.values()]\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(i+1, n_models):\n",
    "            Q = calculate_Q_statistic(predictions[i], predictions[j])\n",
    "            Q_values.append(Q)\n",
    "            \n",
    "    return np.mean(Q_values)\n",
    "\n",
    "def prune_round(models_in, X, y):\n",
    "    \"\"\"Perform a single round of pruning based on Q-statistic diversity\"\"\"\n",
    "    baseline_acc = evaluate_ensemble(models_in, X, y)\n",
    "    baseline_Q = mean_Q_statistic(models_in, X, y)\n",
    "    best_score = baseline_acc\n",
    "    removed = None\n",
    "    # Try removing each model and evaluate both accuracy and diversity\n",
    "    for m in models_in:\n",
    "        dup = models_in.copy()\n",
    "        dup.remove(m)\n",
    "        \n",
    "        # Calculate new accuracy and Q-statistic\n",
    "        new_acc = evaluate_ensemble(dup, X, y)\n",
    "        new_Q = mean_Q_statistic(dup, X, y)\n",
    "        \n",
    "        # Accept removal if accuracy doesn't decrease significantly (within 1%)\n",
    "        # and diversity improves (lower Q-statistic)\n",
    "        if new_acc >= best_score * 0.99 and new_Q < baseline_Q:\n",
    "            best_score = new_acc\n",
    "            removed = m\n",
    "            baseline_Q = new_Q\n",
    "            \n",
    "    return best_score, removed, baseline_Q\n",
    "\n",
    "# prune an ensemble from scratch\n",
    "def prune_ensemble(models, X, y):\n",
    "    scores = []\n",
    "    Q_stats = []\n",
    "    best_score = 0.0\n",
    "    m_length = len(models)-1\n",
    "    iterations = 0\n",
    "    # prune ensemble until no further improvement or max iterations reached\n",
    "    while iterations < m_length:\n",
    "        # remove one model from the ensemble\n",
    "        score, removed, stat_Q = prune_round(models, X, y)\n",
    "        scores.append(score)\n",
    "        Q_stats.append(stat_Q)\n",
    "        # check for no improvement\n",
    "        if removed is None:\n",
    "            print('>no further improvement')\n",
    "            break\n",
    "        # keep track of best score\n",
    "        best_score = score\n",
    "        # remove model from the list\n",
    "        models.remove(removed)\n",
    "        # report results along the way\n",
    "        print('>%.3f (removed: %s)' % (score, removed[0]))\n",
    "        iterations += 1\n",
    "    return best_score, models, scores, Q_stats\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# convert models dict to list of tuples for VotingClassifier\n",
    "models = [(name, model) for name, model in models.items()]\n",
    "score, model_list, scores, Q_stats = prune_ensemble(models, X, y)\n",
    "# Plot scores and Q_stats as time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scores, label='Accuracy Scores', linestyle='-', linewidth=2, color='red')\n",
    "plt.plot(Q_stats, label='Q-Statistics', linestyle='-', linewidth=2, color='blue')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Accuracy Scores and Q-Statistics Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "names = ','.join([n for n, _ in model_list])\n",
    "print('Models: %s' % names)\n",
    "print('Final Mean Accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355603a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
