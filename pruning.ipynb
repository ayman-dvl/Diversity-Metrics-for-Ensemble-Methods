{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6454b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "import os   \n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3010097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\treturn X, y\n",
    "\n",
    "# Charger les modÃ¨les depuis le dossier \"models\"\n",
    "def get_models():\n",
    "    \"\"\"Load models from the 'models' directory\"\"\"\n",
    "    models = {}\n",
    "    models_folder = os.path.join(os.getcwd(), \"models\")\n",
    "    for file in os.listdir(models_folder):\n",
    "        model_name = file.split(\".\")[0]\n",
    "        file_extension = file.split(\".\")[-1]\n",
    "        if file_extension == \"pkl\":\n",
    "            models[model_name] = joblib.load(os.path.join(models_folder, file))\n",
    "            print(f\"Imported sklearn model: {model_name}\")\n",
    "        elif file_extension == \"keras\":\n",
    "            models[model_name] = load_model(os.path.join(models_folder, file))\n",
    "            print(f\"Imported keras model: {model_name}\")\n",
    "    print(models)\n",
    "    return models\n",
    "\n",
    "def custom_soft_voting_predict(models, X):\n",
    "    probas = []  # Initialize probas list\n",
    "    for model in models:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probas.append(model.predict_proba(X))\n",
    "        elif isinstance(model, Sequential):  # Handle Keras models\n",
    "            probas.append(model.predict(X))\n",
    "        else:\n",
    "            print(f\"Model {model} does not support predict_proba and will be skipped.\")\n",
    "    if not probas:\n",
    "        raise ValueError(\"No models in the ensemble support predict_proba.\")\n",
    "    avg_proba = np.mean(probas, axis=0)\n",
    "    return np.argmax(avg_proba, axis=1)\n",
    "\n",
    "# evaluate a list of models\n",
    "def evaluate_ensemble(models, X, y):\n",
    "    # check for no models\n",
    "    if len(models) == 0:\n",
    "        return 0.0\n",
    "    model_objects = [model for _, model in models]\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # calculate predictions for each fold\n",
    "    scores = []\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        # train each model\n",
    "        for model in model_objects:\n",
    "            model.fit(X_train, y_train)\n",
    "        # make predictions using custom soft voting\n",
    "        y_pred = custom_soft_voting_predict(model_objects, X_test)\n",
    "        # calculate accuracy\n",
    "        score = mean(y_pred == y_test)\n",
    "        scores.append(score)\n",
    "    # return mean score\n",
    "    return mean(scores)\n",
    "\n",
    "def calculate_Q_statistic(predictions1, predictions2):\n",
    "    \"\"\"Calculate Q-statistic between two classifiers' predictions\"\"\"\n",
    "    N11 = sum((predictions1 == 1) & (predictions2 == 1))\n",
    "    N00 = sum((predictions1 == 0) & (predictions2 == 0))\n",
    "    N10 = sum((predictions1 == 1) & (predictions2 == 0))\n",
    "    N01 = sum((predictions1 == 0) & (predictions2 == 1))\n",
    "    \n",
    "    Q = (N11 * N00 - N10 * N01) / (N11 * N00 + N10 * N01 + 1e-10)\n",
    "    return Q\n",
    "\n",
    "def get_predictions(model, X, y):\n",
    "    \"\"\"Get binary predictions from a model, handling different prediction attributes\"\"\"\n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            # Models with predict_proba method\n",
    "            y_pred = model.predict_proba(X)[:, 1] > 0.5\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            # Models with decision_function method\n",
    "            y_pred = model.decision_function(X) > 0\n",
    "        else:\n",
    "            # Fallback to predict method\n",
    "            y_pred = model.predict(X)\n",
    "            if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "                y_pred = y_pred[:, 1] > 0.5\n",
    "        return y_pred.astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting with model: {e}\")\n",
    "        # Fallback to zeros if prediction fails\n",
    "        return np.zeros_like(y)\n",
    "\n",
    "def mean_Q_statistic(models, X, y):\n",
    "    \"\"\"Calculate mean Q-statistic across all pairs of models\"\"\"\n",
    "    n_models = len(models)\n",
    "    if n_models < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    Q_values = []\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            pred = get_predictions(model[1], X, y)\n",
    "            predictions.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting predictions for model {model[0]}: {e}\")\n",
    "            predictions.append(np.zeros_like(y))  # Fallback to zeros if prediction fails\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(i+1, n_models):\n",
    "            Q = calculate_Q_statistic(predictions[i], predictions[j])\n",
    "            Q_values.append(Q)\n",
    "            \n",
    "    return np.mean(Q_values)\n",
    "\n",
    "def prune_round(models_in, X, y):\n",
    "    \"\"\"Perform a single round of pruning based on Q-statistic diversity\"\"\"\n",
    "    baseline_acc = evaluate_ensemble(models_in, X, y)\n",
    "    baseline_Q = mean_Q_statistic(models_in, X, y)\n",
    "    best_score = baseline_acc\n",
    "    removed = None\n",
    "    # Try removing each model and evaluate both accuracy and diversity\n",
    "    for m in models_in:\n",
    "        dup = [model for model in models_in if model != m]\n",
    "        \n",
    "        # Calculate new accuracy and Q-statistic\n",
    "        new_acc = evaluate_ensemble(dup, X, y)\n",
    "        new_Q = mean_Q_statistic(dup, X, y)\n",
    "        \n",
    "        # Accept removal if accuracy doesn't decrease significantly (within 1%)\n",
    "        # and diversity improves (lower Q-statistic)\n",
    "        if new_acc >= best_score * 0.99 and new_Q < baseline_Q:\n",
    "            best_score = new_acc\n",
    "            removed = m\n",
    "            baseline_Q = new_Q\n",
    "            \n",
    "    return best_score, removed, baseline_Q\n",
    "\n",
    "# prune an ensemble from scratch\n",
    "def prune_ensemble(models, X, y):\n",
    "    scores = []\n",
    "    Q_stats = []\n",
    "    best_score = 0.0\n",
    "    m_length = len(models)-1\n",
    "    iterations = 0\n",
    "    # prune ensemble until no further improvement or max iterations reached\n",
    "    while iterations < m_length:\n",
    "        # remove one model from the ensemble\n",
    "        score, removed, stat_Q = prune_round(models, X, y)\n",
    "        scores.append(score)\n",
    "        Q_stats.append(stat_Q)\n",
    "        # check for no improvement\n",
    "        if removed is None:\n",
    "            print('>no further improvement')\n",
    "            break\n",
    "        # keep track of best score\n",
    "        best_score = score\n",
    "        models = [model for model in models if model != removed]\n",
    "        models.remove(removed)\n",
    "        # Removed model is already excluded in the list comprehension above\n",
    "        print('>%.3f (removed: %s)' % (score, removed[0]))\n",
    "        iterations += 1\n",
    "    return best_score, models, scores, Q_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d06470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # define dataset\n",
    "    X, y = get_dataset()\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    \n",
    "    # get the models to evaluate\n",
    "    models = get_models()\n",
    "    # Use the predefined models variable\n",
    "    if not models:\n",
    "        raise ValueError(\"No models found in the predefined models list\")\n",
    "    # convert models dict to list of tuples\n",
    "    models = [(name, model) for name, model in models]\n",
    "    print('done')\n",
    "    # run pruning\n",
    "    score, model_list, scores, Q_stats = prune_ensemble(models, X, y)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(scores, label='Accuracy Scores', linestyle='-', linewidth=2, color='red', marker='o')\n",
    "    plt.plot(Q_stats, label='Q-Statistics', linestyle='-', linewidth=2, color='blue', marker='s')\n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Value', fontsize=12)\n",
    "    plt.title('Accuracy Scores and Q-Statistics Over Iterations', fontsize=14, pad=20)\n",
    "    plt.legend(loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Calculate remaining model diversity\n",
    "    final_Q = mean_Q_statistic([model for _, model in model_list], X, y)\n",
    "    print('Final Q-statistic: %.3f' % final_Q)\n",
    "    # Print results\n",
    "    names = ','.join([n for n, _ in model_list])\n",
    "    print('Remaining Models: %s' % names)\n",
    "    print('Final Mean Accuracy: %.3f' % score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fa660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
